<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Face Recognition | weigao</title>
    <meta name="description" content="...">
    <link rel="icon" href="/android-chrome-512x512.png">
  <link rel="manifest" href="/manifest.json">
  <meta name="theme-color" content="#3eaf7c">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <link rel="apple-touch-icon" href="/icons/apple-touch-icon-152x152.png">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#3eaf7c">
  <meta name="msapplication-TileImage" content="/icons/msapplication-icon-144x144.png">
  <meta name="msapplication-TileColor" content="#000000">
    
    <link rel="preload" href="/assets/css/0.styles.e8d92be3.css" as="style"><link rel="preload" href="/assets/js/app.5b882d21.js" as="script"><link rel="preload" href="/assets/js/5.cd1db3e5.js" as="script"><link rel="preload" href="/assets/js/66.60e32eb8.js" as="script"><link rel="preload" href="/assets/js/23.2c687b63.js" as="script"><link rel="prefetch" href="/assets/js/1.61fa752c.js"><link rel="prefetch" href="/assets/js/10.d0a7f483.js"><link rel="prefetch" href="/assets/js/100.c3c38b3f.js"><link rel="prefetch" href="/assets/js/101.ba47a8ea.js"><link rel="prefetch" href="/assets/js/11.40e90e2e.js"><link rel="prefetch" href="/assets/js/12.59472db4.js"><link rel="prefetch" href="/assets/js/13.82dc035e.js"><link rel="prefetch" href="/assets/js/14.3f854a99.js"><link rel="prefetch" href="/assets/js/15.27ecaa65.js"><link rel="prefetch" href="/assets/js/16.7269e8f3.js"><link rel="prefetch" href="/assets/js/17.1baed97f.js"><link rel="prefetch" href="/assets/js/18.337eab3d.js"><link rel="prefetch" href="/assets/js/19.7441a7d8.js"><link rel="prefetch" href="/assets/js/2.abd16662.js"><link rel="prefetch" href="/assets/js/20.3a5f087d.js"><link rel="prefetch" href="/assets/js/21.48fc9301.js"><link rel="prefetch" href="/assets/js/22.82d214b6.js"><link rel="prefetch" href="/assets/js/24.fa6e8fb7.js"><link rel="prefetch" href="/assets/js/25.738cedaa.js"><link rel="prefetch" href="/assets/js/26.c5def0f6.js"><link rel="prefetch" href="/assets/js/27.dc9cdb49.js"><link rel="prefetch" href="/assets/js/28.101b6314.js"><link rel="prefetch" href="/assets/js/29.abf7f7b1.js"><link rel="prefetch" href="/assets/js/30.0629d4d9.js"><link rel="prefetch" href="/assets/js/31.92736f25.js"><link rel="prefetch" href="/assets/js/32.503a383b.js"><link rel="prefetch" href="/assets/js/33.d700f59d.js"><link rel="prefetch" href="/assets/js/34.5304a9af.js"><link rel="prefetch" href="/assets/js/35.ab6a77a1.js"><link rel="prefetch" href="/assets/js/36.603e9dd9.js"><link rel="prefetch" href="/assets/js/37.e89bdebe.js"><link rel="prefetch" href="/assets/js/38.1f73a8c6.js"><link rel="prefetch" href="/assets/js/39.322980cc.js"><link rel="prefetch" href="/assets/js/4.20b55e5f.js"><link rel="prefetch" href="/assets/js/40.4ca46d73.js"><link rel="prefetch" href="/assets/js/41.ff17cbd8.js"><link rel="prefetch" href="/assets/js/42.b93980ad.js"><link rel="prefetch" href="/assets/js/43.625374e9.js"><link rel="prefetch" href="/assets/js/44.1f8b8410.js"><link rel="prefetch" href="/assets/js/45.6b685468.js"><link rel="prefetch" href="/assets/js/46.b1e427dc.js"><link rel="prefetch" href="/assets/js/47.1105348d.js"><link rel="prefetch" href="/assets/js/48.f20ef025.js"><link rel="prefetch" href="/assets/js/49.5b0ccec3.js"><link rel="prefetch" href="/assets/js/50.0d742578.js"><link rel="prefetch" href="/assets/js/51.759bde75.js"><link rel="prefetch" href="/assets/js/52.e961fe01.js"><link rel="prefetch" href="/assets/js/53.cfc3464d.js"><link rel="prefetch" href="/assets/js/54.3add5986.js"><link rel="prefetch" href="/assets/js/55.113f6510.js"><link rel="prefetch" href="/assets/js/56.647d3fbf.js"><link rel="prefetch" href="/assets/js/57.81a8354f.js"><link rel="prefetch" href="/assets/js/58.ac99e744.js"><link rel="prefetch" href="/assets/js/59.e3bb8a78.js"><link rel="prefetch" href="/assets/js/6.46455cb0.js"><link rel="prefetch" href="/assets/js/60.76f1be0c.js"><link rel="prefetch" href="/assets/js/61.547bf157.js"><link rel="prefetch" href="/assets/js/62.bb2d5f35.js"><link rel="prefetch" href="/assets/js/63.1d014a02.js"><link rel="prefetch" href="/assets/js/64.26d8c9ff.js"><link rel="prefetch" href="/assets/js/65.79722d0a.js"><link rel="prefetch" href="/assets/js/67.e94e5af8.js"><link rel="prefetch" href="/assets/js/68.0c62bc2d.js"><link rel="prefetch" href="/assets/js/69.35a9527f.js"><link rel="prefetch" href="/assets/js/7.64614a42.js"><link rel="prefetch" href="/assets/js/70.48adc3c5.js"><link rel="prefetch" href="/assets/js/71.bc9001ba.js"><link rel="prefetch" href="/assets/js/72.677442bb.js"><link rel="prefetch" href="/assets/js/73.5e19868d.js"><link rel="prefetch" href="/assets/js/74.9b41ca8d.js"><link rel="prefetch" href="/assets/js/75.66697b03.js"><link rel="prefetch" href="/assets/js/76.b8a63a09.js"><link rel="prefetch" href="/assets/js/77.5bd5aa5a.js"><link rel="prefetch" href="/assets/js/78.26da0d5f.js"><link rel="prefetch" href="/assets/js/79.5012579e.js"><link rel="prefetch" href="/assets/js/8.11937928.js"><link rel="prefetch" href="/assets/js/80.9042b852.js"><link rel="prefetch" href="/assets/js/81.f18600b7.js"><link rel="prefetch" href="/assets/js/82.81cf8d7b.js"><link rel="prefetch" href="/assets/js/83.368be746.js"><link rel="prefetch" href="/assets/js/84.0ad20007.js"><link rel="prefetch" href="/assets/js/85.bed6c2b0.js"><link rel="prefetch" href="/assets/js/86.a567014d.js"><link rel="prefetch" href="/assets/js/87.9001eb19.js"><link rel="prefetch" href="/assets/js/88.9133eeba.js"><link rel="prefetch" href="/assets/js/89.6d4e53cb.js"><link rel="prefetch" href="/assets/js/9.d01dceaa.js"><link rel="prefetch" href="/assets/js/90.865b8b70.js"><link rel="prefetch" href="/assets/js/91.18aafcc1.js"><link rel="prefetch" href="/assets/js/92.cf3382cb.js"><link rel="prefetch" href="/assets/js/93.b6003c1b.js"><link rel="prefetch" href="/assets/js/94.84d5e718.js"><link rel="prefetch" href="/assets/js/95.eb85deaa.js"><link rel="prefetch" href="/assets/js/96.568ffb46.js"><link rel="prefetch" href="/assets/js/97.8b1d193f.js"><link rel="prefetch" href="/assets/js/98.c7ac97e9.js"><link rel="prefetch" href="/assets/js/99.5dc9efbf.js">
    <link rel="stylesheet" href="/assets/css/0.styles.e8d92be3.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">weigao</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">Home</a></div><div class="nav-item"><a href="/algorithm/" class="nav-link">Algorithm</a></div><div class="nav-item"><a href="/commits/" class="nav-link">Commits</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">Home</a></div><div class="nav-item"><a href="/algorithm/" class="nav-link">Algorithm</a></div><div class="nav-item"><a href="/commits/" class="nav-link">Commits</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Face Recognition</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/blog/Deeplearning/face_recognition.html#_1-abstract" class="sidebar-link">1. Abstract</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/blog/Deeplearning/face_recognition.html#_2-yolo" class="sidebar-link">2. YOLO</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/blog/Deeplearning/face_recognition.html#_2-1-sliding-window" class="sidebar-link">2.1 Sliding window</a></li><li class="sidebar-sub-header"><a href="/blog/Deeplearning/face_recognition.html#_2-2-yolo" class="sidebar-link">2.2 YOLO</a></li></ul></li><li><a href="/blog/Deeplearning/face_recognition.html#_3-face-recognition" class="sidebar-link">3. Face recognition</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/blog/Deeplearning/face_recognition.html#_3-1-dataset" class="sidebar-link">3.1 Dataset</a></li><li class="sidebar-sub-header"><a href="/blog/Deeplearning/face_recognition.html#_3-2-darknet" class="sidebar-link">3.2 Darknet</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="content default"><h1 id="face-recognition"><a href="#face-recognition" aria-hidden="true" class="header-anchor">#</a> Face Recognition</h1> <p>Face recognition based on YOLO, <strong>You Only Look Once: Unified, Real-Time Object Detection</strong>.</p> <h2 id="_1-abstract"><a href="#_1-abstract" aria-hidden="true" class="header-anchor">#</a> 1. Abstract</h2> <p>自YOLO算法提出以来，至今已经发展到了v3，性能、集成性等都得到了极大的提升，用YOLO来实现人脸识别算法，其特点是模型训练参数较少，可移植并且实时性很高。目前为止，集成现有技术实现一个基于YOLO算法的人脸识别系统是一项很有挑战性的工作。近几年来，目标检测算法取得了很大的突破。比较流行的算法可以分为两类，一类是基于Region Proposal的R-CNN系算法（R-CNN，Fast R-CNN, Faster R-CNN），它们是two-stage的，需要先使用启发式方法（selective search）或者CNN网络（RPN）产生Region Proposal，然后再在Region Proposal上做分类与回归。另一类是Yolo，SSD这类one-stage算法，其仅仅使用一个CNN网络直接预测不同目标的类别与位置。第一类方法准确度高，但是速度慢，第二类算法速度快，但是准确性较低。本文将介绍Yolo算法，其全称是<strong>You Only Look Once: Unified, Real-Time Object Detection</strong>，You Only Look Once说的是只需要一次CNN运算，Unified指的是这是一个统一的框架，提供end-to-end的预测，而Real-Time体现是Yolo算法速度快。这里我们谈的是Yolo-v1版本算法，其性能差于后来的SSD算法的，但是Yolo后来也继续进行改进，产生了Yolo9000算法。本文主要讲述Yolo-v1算法的原理。</p> <h2 id="_2-yolo"><a href="#_2-yolo" aria-hidden="true" class="header-anchor">#</a> 2. YOLO</h2> <h3 id="_2-1-sliding-window"><a href="#_2-1-sliding-window" aria-hidden="true" class="header-anchor">#</a> 2.1 Sliding window</h3> <blockquote><p>The sliding window model is conceptually simple: independently classify all image patches as being object or non-object. Sliding window classification is the dominant paradigm in object detection and for one object category in particular -- faces -- it is one of the most noticeable successes of computer vision. For example, modern cameras and photo organization tools have prominent face detection capabilities.</p></blockquote> <p>如引文所示，常见的目标检测算法一般基于滑动窗口模型，其思路是，将检测问题转化为了图像分类问题。其基本原理就是采用不同大小和窗口在整张图片上以一定的步长进行滑动，然后对这些窗口对应的区域做图像分类，以此实现对整张图片的检测。</p> <p>但该方法的缺点在于，要检测的目标大小规模是未知的，所以在实验中需要设置不同大小的窗口去滑动，并且还要选取合适的步长。这样的做法会产生很多的子区域经过分类器去做预测，需要很大的计算量，但是为了保证速度，分类器的设计不能过于复杂。可以想到的解决思路之一是减少要分类的子区域，这是R-CNN的一个改进策略，其采用了selective search方法来找到最有可能包含目标的子区域（Region Proposal）。</p> <p>对于CNN分类器而言，滑动窗口是非常耗时的，幸运的是，结合卷积运算的特点，我们可以使用CNN实现更高效的滑动窗口方法。有一种全卷积的方法，用卷积层代替了全连接层。overfeat算法的思路是，如果输入图片大小是16x16，经过一系列卷积操作，提取了2x2的特征图，这个2x2的图上每个元素都是和原图是一一对应的，相当于在原图上做大小为14x14的窗口滑动，且步长为2，共产生4个子区域，最终输出的通道数为4，可以看成4个类别的预测概率值，这样一次CNN计算就可以实现窗口滑动的所有子区域的分类预测。由于图片的空间位置信息的不变性，尽管卷积过程中图片大小减少，但是位置对应关系还是保存的。这个思路也被R-CNN借鉴，从而诞生了Fast R-CNN算法。</p> <p>上面尽管可以减少滑动窗口的计算量，但是只是针对一个固定大小与步长的窗口，这是远远不够的。Yolo算法很好的解决了这个问题，它不再是窗口滑动了，而是直接将原始图片分割成互不重合的小方块，然后通过卷积最后生产这样大小的特征图，基于上面的分析，可以认为特征图的每个元素也是对应原始图片的一个小方块，然后用每个元素来可以预测那些中心点在该小方格内的目标，这就是Yolo算法的朴素思想。</p> <h3 id="_2-2-yolo"><a href="#_2-2-yolo" aria-hidden="true" class="header-anchor">#</a> 2.2 YOLO</h3> <h4 id="_2-2-1-unified-detection"><a href="#_2-2-1-unified-detection" aria-hidden="true" class="header-anchor">#</a> 2.2.1 Unified Detection</h4> <p>整体来看，Yolo算法采用一个单独的CNN模型实现end-to-end的目标检测，整个系统如图所示：首先将输入图片resize到448x448，然后送入CNN网络，最后处理网络预测结果得到检测的目标。相比R-CNN算法，其是一个统一的框架，其速度更快，而且Yolo的训练过程也是end-to-end的。</p> <p>具体来说，Yolo的CNN网络将输入的图片分割成<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>∗</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">S*S</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05764em;">S</span><span class="mbin">∗</span><span class="mord mathit" style="margin-right:0.05764em;">S</span></span></span></span>网格(<strong>grid</strong>)，然后每个单元格负责去检测那些中心点落在该格子内的目标，如下图所示，可以看到狗这个目标的中心落在左下角一个单元格内，那么该单元格负责预测这个狗。每个单元格会预测<strong>B</strong>个边界框（<strong>bounding box</strong>）以及边界框的置信度（confidence score），置信度包含两个方面，一是这个边界框含有目标的可能性大小$$Pr(object)$$，二是这个边界框的准确度。</p> <ul><li>当该边界框是背景时（即不包含目标），$$Pr(object)=0$$。</li> <li>当该边界框包含目标时，$$Pr(object)=1$$。</li></ul> <p>边界框的准确度可以用预测框与实际框（ground truth）的**IOU（intersection over union，交并比）**来表征。因此置信度可以定义为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mi>r</mi><mo>(</mo><mi>o</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mo>)</mo><mo>∗</mo><mi>I</mi><mi>O</mi><mi>U</mi></mrow><annotation encoding="application/x-tex">Pr(object)*IOU</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathit">o</span><span class="mord mathit">b</span><span class="mord mathit" style="margin-right:0.05724em;">j</span><span class="mord mathit">e</span><span class="mord mathit">c</span><span class="mord mathit">t</span><span class="mclose">)</span><span class="mbin">∗</span><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mord mathit" style="margin-right:0.02778em;">O</span><span class="mord mathit" style="margin-right:0.10903em;">U</span></span></span></span>。Yolo的置信度不仅仅是边界框是否含有目标的概率，而是两个因子的乘积，预测框的准确度也反映在该乘积中。</p> <p>边界框的大小与位置可以用4个值来表征：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(x,y,h,w)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mpunct">,</span><span class="mord mathit">h</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span>，其中$$(x,y)$$是边界框的中心坐标，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(h,w)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord mathit">h</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span>是边界框的宽与高。中心坐标的预测值$$(x,y)$$是相对于每个单元格左上角坐标点的偏移值，单位是相对于单元格大小的，而边界框的w和h预测值是相对于整个图片的宽与高的比例，这样理论上4个元素的大小应该在[0,1]范围。最终，每个边界框的预测值实际上包含5个元素：$$(x,y,w,h,c)$$，其中前4个表征边界框的大小与位置，而最后一个值是置信度。</p> <p>每个单元格需要预测$$(B*5+C)$$个值。如果将输入图片划分为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>∗</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">S*S</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05764em;">S</span><span class="mbin">∗</span><span class="mord mathit" style="margin-right:0.05764em;">S</span></span></span></span>网格，那么最终预测值为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>∗</mo><mi>S</mi><mo>∗</mo><mo>(</mo><mi>B</mi><mo>∗</mo><mn>5</mn><mo>+</mo><mi>C</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">S*S*(B*5+C)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05764em;">S</span><span class="mbin">∗</span><span class="mord mathit" style="margin-right:0.05764em;">S</span><span class="mbin">∗</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.05017em;">B</span><span class="mbin">∗</span><span class="mord mathrm">5</span><span class="mbin">+</span><span class="mord mathit" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span>大小的张量。整个模型的预测值结构如下图所示。对于PASCALVOC数据，其共有20个类别，如果使用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>=</mo><mn>7</mn><mo separator="true">,</mo><mi>B</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">S=7,B=2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.05764em;">S</span><span class="mrel">=</span><span class="mord mathrm">7</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.05017em;">B</span><span class="mrel">=</span><span class="mord mathrm">2</span></span></span></span>,那么最终的预测结果就是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>7</mn><mo>∗</mo><mn>7</mn><mo>∗</mo><mn>3</mn><mn>0</mn></mrow><annotation encoding="application/x-tex">7*7*30</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathrm">7</span><span class="mbin">∗</span><span class="mord mathrm">7</span><span class="mbin">∗</span><span class="mord mathrm">3</span><span class="mord mathrm">0</span></span></span></span>大小的张量。</p> <h4 id="_2-2-2-design"><a href="#_2-2-2-design" aria-hidden="true" class="header-anchor">#</a> 2.2.2 Design</h4> <p>Yolo采用卷积网络来提取特征，然后使用全连接层来得到预测值。网络结构参考GooLeNet模型，包含24个卷积层和2个全连接层。对于卷积层，主要使用1x1卷积来做channle reduction，然后紧跟3x3卷积。对于卷积层和全连接层，采用Leaky ReLU激活函数：$$max(x,0)$$。最后一层``采用线性激活函数。除了上面这个结构，文章还提出了一个轻量级版本Fast Yolo，其仅使用9个卷积层，并且卷积层中使用更少的卷积核。</p> <p>在训练之前，先在ImageNet上进行了预训练，其预训练的分类模型采用图中前20个卷积层，然后添加一个average-pool层和全连接层。预训练之后，在预训练得到的20层卷积层之上加上随机初始化的4个卷积层和2个全连接层。由于检测任务一般需要更高清的图片，所以将网络的输入从224x224增加到了448x448。</p> <p>由于每个单元格预测多个边界框。但是其对应类别只有一个。那么在训练时，如果该单元格内确实存在目标，那么只选择与ground truth的IOU较大的那个边界框来负责预测该目标，而其它边界框认为不存在目标。这样设置的一个结果将会使一个单元格对应的边界框更加专业化，其可以分别适用不同大小，不同高宽比的目标，从而提升模型性能。大家可能会想如果一个单元格内存在多个目标怎么办，其实这时候Yolo算法就只能选择其中一个来训练，这也是Yolo算法的缺点之一。要注意的一点时，对于不存在对应目标的边界框，其误差项就是只有置信度，左标项误差是没法计算的。而只有当一个单元格内确实存在目标时，才计算分类误差项，否则该项也是无法计算的。</p> <h4 id="_2-2-3-nms"><a href="#_2-2-3-nms" aria-hidden="true" class="header-anchor">#</a> 2.2.3 NMS</h4> <p>NMS即非极大值抑制算法（non maximum suppression, NMS），这个算法不单单是针对Yolo算法的，而是所有的检测算法中都会用到。NMS算法主要解决的是一个目标被多次检测的问题，如人脸检测的案例中，人脸可能被多次检测，但是其实预测只希望最后仅仅输出其中一个较好的预测框，那么可以采用NMS算法来实现这样的效果：首先从所有的检测框中找到置信度较大的那个框，然后挨个计算其与剩余框的IOU，如果其值大于一定阈值（重合度过高），那么就将该框剔除；然后对剩余的检测框重复上述过程，直到处理完所有的检测框。Yolo预测过程也需要用到NMS算法。</p> <p>对于Yolo的预测过程，首先不考虑batch，认为只是预测一张输入图片。根据前面的分析，最终的网络输出是7<em>7</em>30，但是我们可以将其分割成三个部分：类别概率部分为$$[7,7,20]$$，置信度部分为$$[7,7,2,2]$$，而边界框部分为$$[7,7,2,4]$$。然后将前两项相乘可以得到类别置信度值为$$[7,7,2,20]$$，这里总共预测了$$7<em>7</em>2=98$$边界框。</p> <p>所有的准备数据已经得到了，对于第一种策略来得到检测框的结果，我认为这是最正常与自然的处理。首先，对于每个预测框根据类别置信度选取置信度较大的那个类别作为其预测标签，经过这层处理我们得到各个预测框的预测类别及对应的置信度值，其大小都是$$[7,7,2]$$。一般情况下，会设置置信度阈值，就是将置信度小于该阈值的box过滤掉，所以经过这层处理，剩余的是置信度比较高的预测框。最后再对这些预测框使用NMS算法，最后留下来的就是检测结果。一个值得注意的点是NMS是对所有预测框一视同仁，还是区分每个类别，分别使用NMS。Ng应该区分每个类别分别使用NMS，但是实际的应用中，还是同等对待所有的框。</p> <p>对于Yolo算法，其采用了另外一个不同的处理思路，与上段所述方法其区别就是先使用NMS，然后再确定各个box的类别。对于每个boxes，首先将小于置信度阈值的值归0，然后分类别地对置信度值采用NMS，这里NMS处理结果不是剔除，而是将其置信度值归为0。最后才是确定各个box的类别，当其置信度值不为0时才做出检测结果输出。我们需要慎重选择NMS算法，因为其对Yolo的性能是影响很大的。</p> <h2 id="_3-face-recognition"><a href="#_3-face-recognition" aria-hidden="true" class="header-anchor">#</a> 3. Face recognition</h2> <p>该章主要介绍如何利用YOLO算法实现人脸检测的功能，主要为代码的解析和实验说明。</p> <h3 id="_3-1-dataset"><a href="#_3-1-dataset" aria-hidden="true" class="header-anchor">#</a> 3.1 Dataset</h3> <p>YOLO本身使用的是VOC的数据集，但是我们的实验中需要使用自定义数据集来训练YOLO网络的话，就需要在VOC数据集基础上进行改造，按照VOC数据集的结构和格式来构建所需的数据集。在本实验中使用的数据集是CelebA大规模名人人脸标注数据集，CelebA数据集中的图片命名格式为统一的000001.jpg—202599.jpg，BoundingBox的信息在list_bbox_celeba.txt中保存。</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token comment"># list_bbox_celeba.txt</span>
line 1: The number of picture
line 2: information
line 3: <span class="token operator">&lt;</span> xxxxxx.jpg <span class="token operator">&gt;</span> <span class="token operator">&lt;</span> <span class="token punctuation">[</span>x1<span class="token punctuation">]</span> <span class="token punctuation">[</span>y1<span class="token punctuation">]</span> <span class="token punctuation">[</span>width<span class="token punctuation">]</span> <span class="token punctuation">[</span>height<span class="token punctuation">]</span> <span class="token operator">&gt;</span>
</code></pre></div><p>其中$$x_1$$，$$y_1$$表示该BoundingBox左上角点的坐标，$$width$$，$$height$$分别表示该BoundingBox的宽度和高度。 在YOLO中，每张图片都需要一个对应的label文件，这个label文件应当是一个.txt文件，除后缀名外，它的文件名与该图片的文件名相同，其中的内容为&lt; 类别代码&gt;  &lt; $$[x][y][width][height] $$&gt;，类别代码为从0开始的整数，它用于在.names文件中指明该BoundingBox中目标的类别。这里x = BoundingBox的中心点横坐标/图片宽度，y = BoundingBox的中心点纵坐标/图片高度，width = BoundingBox宽度/图片宽度，height = BoundingBox高度/图片高度。</p> <p>YOLO训练时需要：</p> <ol><li><pre><code> 指明训练图片绝对路径的train.txt文件和指明验证图片绝对路径的val.txt。
</code></pre></li> <li><pre><code> 所有图片对应的label文本文件，在voc数据集中位于VOC2007/labels文件夹内。
</code></pre></li> <li><pre><code> 训练数据配置文件voc.data
</code></pre></li> <li><pre><code> 网络配置文件，这里用tiny-yolo.cfg
</code></pre></li> <li><pre><code> 类别名列表文件voc.names
</code></pre></li></ol> <p>其中1,2是由我们自己生成赖江celebA的数据集转化成VOC的数据集，而后对目录结构进行调整。</p> <h3 id="_3-2-darknet"><a href="#_3-2-darknet" aria-hidden="true" class="header-anchor">#</a> 3.2 Darknet</h3> <p>Darknet是一个开源的神经网络框架，其使用C和CUDA。</p> <h4 id="_3-2-1-installing-darknet"><a href="#_3-2-1-installing-darknet" aria-hidden="true" class="header-anchor">#</a> 3.2.1 Installing Darknet</h4> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token function">git</span> clone https://github.com/pjreddie/darknet.git
<span class="token function">cd</span> darknet
<span class="token function">make</span>
</code></pre></div><p>执行上述代码安装，我们使用的是ubuntu 18.04LTS操作系统。</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token function">wget</span> https://pjreddie.com/media/files/yolov3.weights
</code></pre></div><p>下载好提前训练好的参数，然后就可以运行detector了。</p> <div class="language-bash extra-class"><pre class="language-bash"><code>./darknet detect cfg/yolov3.cfg yolov3.weights data/horses.jpg
</code></pre></div><p>运行后会产生如下的输出：</p> <div class="language-bash extra-class"><pre class="language-bash"><code>layer     filters    size              input                output
    0 conv     32  3 x 3 / 1   416 x 416 x   3   -<span class="token operator">&gt;</span>   416 x 416 x  32  0.299 BFLOPs
    1 conv     64  3 x 3 / 2   416 x 416 x  32   -<span class="token operator">&gt;</span>   208 x 208 x  64  1.595 BFLOPs
    <span class="token punctuation">..</span>.
  104 conv    256  3 x 3 / 1    52 x  52 x 128   -<span class="token operator">&gt;</span>    52 x  52 x 256  1.595 BFLOPs
  105 conv    255  1 x 1 / 1    52 x  52 x 256   -<span class="token operator">&gt;</span>    52 x  52 x 255  0.353 BFLOPs
  106 yolo
Loading weights from yolov3.weights<span class="token punctuation">..</span>.Done<span class="token operator">!</span>
data/dog.jpg: Predicted <span class="token keyword">in</span> 21.323896 seconds.
dog: 99%
truck: 92%
bicycle: 99%
</code></pre></div><p>输出的预测图片为predictions.png:</p> <h4 id="_3-2-2-face-recognition"><a href="#_3-2-2-face-recognition" aria-hidden="true" class="header-anchor">#</a> 3.2.2 Face recognition</h4> <p>该系统的设计步骤为：在上节下载好的<code>darknet</code>文件夹中进行一些文件和脚本的修改，以便于我们进行人脸识别。</p> <ol><li><p>创建数据集文件；</p></li> <li><p>采集需要识别的头像，放入文件夹中；</p></li> <li><p>标注人名为类别名，基于labelImg，其中类别名称为人名，并使用脚本生成文件名称列表；</p></li> <li><p>修改<code>script</code>文件夹下面的<code>voc_label.py</code>文件，执行过后，会生成标注文件对应的txt文件；</p></li> <li><p>修改配置文件<code>./cfg/voc.data</code>中的内容</p></li> <li><div class="language-bash extra-class"><pre class="language-bash"><code>classes<span class="token operator">=</span> 4
train  <span class="token operator">=</span> /home/user/darknet/scripts/homework_train.txt
//valid  <span class="token operator">=</span> /home/pjreddie/data/voc/homework_test.txt
names <span class="token operator">=</span> data/voc.names
backup <span class="token operator">=</span> /home/user/darknet/result
</code></pre></div></li> <li><p>修改``./data/voc.names`，将四个类别名称（人名）写进，原来的都删除掉。;</p></li> <li><p>修改网络参数的最后一层，在文件<code>tiny-yolo-voc.cfg</code>中，修改如下：</p></li> <li><div class="language-python extra-class"><pre class="language-python"><code><span class="token punctuation">.</span>
<span class="token punctuation">.</span>
<span class="token punctuation">.</span>
<span class="token punctuation">[</span>convolutional<span class="token punctuation">]</span>
size<span class="token operator">=</span><span class="token number">1</span>
stride<span class="token operator">=</span><span class="token number">1</span>
pad<span class="token operator">=</span><span class="token number">1</span>
filters<span class="token operator">=</span><span class="token number">45</span>
activation<span class="token operator">=</span>linear
 
<span class="token punctuation">[</span>region<span class="token punctuation">]</span>
anchors <span class="token operator">=</span> <span class="token number">1.08</span><span class="token punctuation">,</span><span class="token number">1.19</span><span class="token punctuation">,</span>  <span class="token number">3.42</span><span class="token punctuation">,</span><span class="token number">4.41</span><span class="token punctuation">,</span>  <span class="token number">6.63</span><span class="token punctuation">,</span><span class="token number">11.38</span><span class="token punctuation">,</span>  <span class="token number">9.42</span><span class="token punctuation">,</span><span class="token number">5.11</span><span class="token punctuation">,</span>  <span class="token number">16.62</span><span class="token punctuation">,</span><span class="token number">10.52</span>
bias_match<span class="token operator">=</span><span class="token number">1</span>
classes<span class="token operator">=</span><span class="token number">4</span>   
coords<span class="token operator">=</span><span class="token number">4</span>
num<span class="token operator">=</span><span class="token number">5</span>
softmax<span class="token operator">=</span><span class="token number">1</span>
jitter<span class="token operator">=</span><span class="token number">.2</span>
rescore<span class="token operator">=</span><span class="token number">1</span>
 
object_scale<span class="token operator">=</span><span class="token number">5</span>
noobject_scale<span class="token operator">=</span><span class="token number">1</span>
class_scale<span class="token operator">=</span><span class="token number">1</span>
coord_scale<span class="token operator">=</span><span class="token number">1</span>
 
absolute<span class="token operator">=</span><span class="token number">1</span>
thresh <span class="token operator">=</span> <span class="token number">.6</span>
random<span class="token operator">=</span><span class="token number">1</span>
</code></pre></div><p>训练网络</p> <div class="language- extra-class"><pre class="language-text"><code>./darknet detector train ./cfg/voc.data cfg/tiny-yolo-voc.cfg  
</code></pre></div></li> <li><p>在漫长的等待过后，可以在<code>./darknet/results</code>生成最后的参数数据，其中<code>tiny-yolo-voc_final.weights</code>是最后的参数，其他的都是过程参数。</p></li> <li><p>进行测试</p></li> <li><div class="language- extra-class"><pre class="language-text"><code>./darknet detector test cfg/voc.data cfg/tiny-yolo-voc.cfg 
results/tiny-yolo-voc_final.weights data/images.jpg
</code></pre></div><p>至此，整个过程完成。</p></li></ol></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">4/28/2019, 5:57:40 PM</span></div></footer> <!----> </main></div><div class="global-ui"><!----><!----></div></div>
    <script src="/assets/js/app.5b882d21.js" defer></script><script src="/assets/js/5.cd1db3e5.js" defer></script><script src="/assets/js/66.60e32eb8.js" defer></script><script src="/assets/js/23.2c687b63.js" defer></script>
  </body>
</html>
